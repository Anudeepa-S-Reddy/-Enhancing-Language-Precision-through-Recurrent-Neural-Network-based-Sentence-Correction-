{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZZZv3cClLHWc"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 930,
     "status": "ok",
     "timestamp": 1620719760496,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "1rc-mJHC0msT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20759,
     "status": "ok",
     "timestamp": 1620719683026,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "5awZgGMXi7tr",
    "outputId": "2204af32-452b-4aa7-b6d3-ab0476ae9194"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive/\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18420,
     "status": "ok",
     "timestamp": 1620719683026,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "bLikn0SA03Ke",
    "outputId": "aec34ded-583e-40da-961e-3c9f3ebb18d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/MyDrive/CS_2\n"
     ]
    }
   ],
   "source": [
    "%cd /content/gdrive/MyDrive/CS_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4pfGNaWFGG1"
   },
   "source": [
    "# Word_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ib48UQJdNrry"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1250,
     "status": "ok",
     "timestamp": 1620719687502,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "AHvuGeSmi76r"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('data_wl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 909,
     "status": "ok",
     "timestamp": 1620719687869,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "5qSpl-vxi8ck"
   },
   "outputs": [],
   "source": [
    "tr,val = train_test_split(data, test_size=0.01) # as we have very less data to work with, we use major portion to train the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 969,
     "status": "ok",
     "timestamp": 1620719689309,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "5iBprWw6zElM"
   },
   "outputs": [],
   "source": [
    "# Creating the word tokens from the sentences\n",
    "tokenizer_raw = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', lower=True)\n",
    "tokenizer_raw.fit_on_texts(tr['Source'].values)\n",
    "\n",
    "tokenizer_std = Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n', lower=True)\n",
    "tokenizer_std.fit_on_texts(tr['Target_inp'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1367,
     "status": "ok",
     "timestamp": 1620719692618,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "T314vkTifMU9"
   },
   "outputs": [],
   "source": [
    "Source_vocab_size=len(tokenizer_raw.word_index.keys())+1 \n",
    "Target_vocab_size=len(tokenizer_std.word_index.keys())+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 1183,
     "status": "ok",
     "timestamp": 1620719693011,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "SRfEe1wy0smY"
   },
   "outputs": [],
   "source": [
    "# Assigning token digits to every word in a sentence\n",
    "Source_seq_tr = tokenizer_raw.texts_to_sequences(tr['Source']) \n",
    "Target_inp_seq_tr = tokenizer_std.texts_to_sequences(tr['Target_inp'])\n",
    "Target_out_seq_tr = tokenizer_std.texts_to_sequences(tr['Target_out'])\n",
    "\n",
    "Source_seq_val = tokenizer_raw.texts_to_sequences(val['Source'])\n",
    "Target_inp_seq_val = tokenizer_std.texts_to_sequences(val['Target_inp'])\n",
    "Target_out_seq_val = tokenizer_std.texts_to_sequences(val['Target_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 677,
     "status": "ok",
     "timestamp": 1620719693731,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "JIVkK4RxY4wY"
   },
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in Source_seq_tr:\n",
    "    l.append(len(i))\n",
    "max_len=max(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1714,
     "status": "ok",
     "timestamp": 1620719695416,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "5bAaebk-6qR2"
   },
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in Target_inp_seq_tr:\n",
    "    l.append(len(i))\n",
    "max_len_dec_ip=max(l)\n",
    "\n",
    "l=[]\n",
    "for i in Target_out_seq_tr:\n",
    "    l.append(len(i))\n",
    "max_len_dec_op=max(l)\n",
    "\n",
    "max_len_dec=max(max_len_dec_ip,max_len_dec_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 611,
     "status": "ok",
     "timestamp": 1620719695417,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "3fmE5ypHcvJP"
   },
   "outputs": [],
   "source": [
    "#Padding the sentences with zeros so that all sentences are of equal length\n",
    "target_inp_tr = pad_sequences(Target_inp_seq_tr,  maxlen=max_len_dec, padding='post')\n",
    "target_out_tr = pad_sequences(Target_out_seq_tr, maxlen=max_len_dec, padding='post')\n",
    "Source_inp_tr = pad_sequences(Source_seq_tr, maxlen=max_len, padding='post')\n",
    "\n",
    "target_inp_val = pad_sequences(Target_inp_seq_val,  maxlen=max_len_dec, padding='post')\n",
    "target_out_val = pad_sequences(Target_out_seq_val, maxlen=max_len_dec, padding='post')\n",
    "Source_inp_val = pad_sequences(Source_seq_val, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xDhHIAMYNySb"
   },
   "source": [
    "### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 1251,
     "status": "ok",
     "timestamp": 1620719697165,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "AdFUv8_QeUw1"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units,input_length):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.enc_units= enc_units\n",
    "        self.lstm_output = 0\n",
    "        self.lstm_state_h=0\n",
    "        self.lstm_state_c=0\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim,trainable=True,input_length=self.input_length,\n",
    "                           mask_zero=True, name=\"embedding_layer_encoder\")\n",
    "        self.lstm = LSTM(self.enc_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
    "        \n",
    "    def call(self, input_sentances, training=True):\n",
    "        input_embedd                           = self.embedding(input_sentances)\n",
    "        self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.lstm(input_embedd)\n",
    "        return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
    "\n",
    "    def initialize_states(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.enc_units)), tf.zeros((batch_size, self.enc_units))\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.lstm_state_h,self.lstm_state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 882,
     "status": "ok",
     "timestamp": 1620719701446,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "D1LyDCWCzQxq"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim,dec_units, input_length):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dec_units = dec_units\n",
    "        self.input_length = input_length\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim,input_length=self.input_length,\n",
    "                           mask_zero=True, name=\"embedding_layer_decoder\",trainable=True)\n",
    "        self.lstm = LSTM(self.dec_units, return_sequences=True, return_state=True, name=\"Encoder_LSTM\")\n",
    "    \n",
    "    def call(self, target_sentances,states):\n",
    "        target_embedd           = self.embedding(target_sentances)\n",
    "        lstm_output,decoder_final_state_h,decoder_final_state_c      = self.lstm(target_embedd, initial_state=states)\n",
    "        return  lstm_output,decoder_final_state_h,decoder_final_state_c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 1107,
     "status": "ok",
     "timestamp": 1620719703069,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "JH3Ekhku36zS"
   },
   "outputs": [],
   "source": [
    "class Encoder_decoder(Model):\n",
    "    def __init__(self, encoder_inputs_length,decoder_inputs_length, output_vocab_size,batch_size):\n",
    "        super().__init__() # https://stackoverflow.com/a/27134600/4084039\n",
    "        self.encoder = Encoder(vocab_size=Source_vocab_size, embedding_dim=50, input_length=encoder_inputs_length, enc_units=256)\n",
    "        self.decoder = Decoder(vocab_size=Target_vocab_size, embedding_dim=100, input_length=decoder_inputs_length, dec_units=256)\n",
    "        self.dense   = Dense(output_vocab_size)\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "    def call(self, data):\n",
    "        input,output = data[0], data[1]\n",
    "        enc_initial_states = self.encoder.initialize_states(self.batch_size)\n",
    "        encoder_output, encoder_h, encoder_c = self.encoder(input,enc_initial_states)\n",
    "        states=[encoder_h, encoder_c]\n",
    "        decoder_output,decoder_final_state_h,decoder_final_state_c    = self.decoder(output,states)\n",
    "        output                               = self.dense(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 6094,
     "status": "ok",
     "timestamp": 1620719709316,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "6Eh59rgH382x"
   },
   "outputs": [],
   "source": [
    "model  = Encoder_decoder(encoder_inputs_length=max_len,decoder_inputs_length=max_len_dec,output_vocab_size=Source_vocab_size, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 1102,
     "status": "ok",
     "timestamp": 1620719895296,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "JcC7EeYnXIlz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "checkpoint_1 = keras.callbacks.ModelCheckpoint('weights', save_best_only= True, monitor='val_loss', mode = 'min', verbose= 1)\n",
    "tensorboard_callback = keras.callbacks.TensorBoard('log', histogram_freq=0)\n",
    "\n",
    "# Reduce learning rate based on the validation loss\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=1, mode=\"min\", verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 600,
     "status": "ok",
     "timestamp": 1620719904935,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "RcYx7Mdg3--f"
   },
   "outputs": [],
   "source": [
    "def custom_lossfunction(real, pred):\n",
    "    #https://www.tensorflow.org/tutorials/text/nmt_with_attention#define_the_optimizer_and_the_loss_function\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "model.compile(optimizer=optimizer,loss=custom_lossfunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdlrij0rN17Q"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33094,
     "status": "ok",
     "timestamp": 1620672200208,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "hqut50zXK8h2",
    "outputId": "c07236e4-c05f-48cb-ff62-6718fff84006"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 10s 477ms/step - loss: 0.9072 - val_loss: 0.6388\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.63883, saving model to weights\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.8805 - val_loss: 0.5744\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.63883 to 0.57441, saving model to weights\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.7616 - val_loss: 0.4779\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.57441 to 0.47789, saving model to weights\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6929 - val_loss: 0.4701\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.47789 to 0.47007, saving model to weights\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.6706 - val_loss: 0.4681\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.47007 to 0.46810, saving model to weights\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6728 - val_loss: 0.4661\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.46810 to 0.46612, saving model to weights\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.6570 - val_loss: 0.4646\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.46612 to 0.46456, saving model to weights\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.6513 - val_loss: 0.4635\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.46456 to 0.46347, saving model to weights\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.6565 - val_loss: 0.4611\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.46347 to 0.46114, saving model to weights\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6570 - val_loss: 0.4610\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.46114 to 0.46103, saving model to weights\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6552 - val_loss: 0.4600\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.46103 to 0.46004, saving model to weights\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6770 - val_loss: 0.4592\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.46004 to 0.45916, saving model to weights\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.6543 - val_loss: 0.4591\n",
      "\n",
      "Epoch 00013: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.45916 to 0.45913, saving model to weights\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6723 - val_loss: 0.4589\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.45913 to 0.45894, saving model to weights\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6467 - val_loss: 0.4587\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.45894 to 0.45871, saving model to weights\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.6508 - val_loss: 0.4584\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.45871 to 0.45841, saving model to weights\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6583 - val_loss: 0.4583\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.45841 to 0.45825, saving model to weights\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.6543 - val_loss: 0.4581\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.45825 to 0.45813, saving model to weights\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6523 - val_loss: 0.4580\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.45813 to 0.45802, saving model to weights\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.6734 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.45802 to 0.45792, saving model to weights\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6561 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.45792 to 0.45792, saving model to weights\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.6662 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.45792 to 0.45791, saving model to weights\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6529 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00023: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.45791 to 0.45791, saving model to weights\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6570 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.45791 to 0.45791, saving model to weights\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6622 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-09.\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.45791\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.6714 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-10.\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.45791\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6722 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00027: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-11.\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.45791\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6378 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-12.\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.45791\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6580 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 1.0000001044244145e-13.\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.45791\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.6516 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00030: ReduceLROnPlateau reducing learning rate to 1.0000001179769417e-14.\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.45791\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6357 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 1.0000001518582595e-15.\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.45791\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6494 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000001095066122e-16.\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.45791\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6409 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 1.0000000830368326e-17.\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.45791\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6606 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 1.0000000664932204e-18.\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.45791\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.6708 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 1.000000045813705e-19.\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.45791\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6786 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 1.000000032889008e-20.\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.45791\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6726 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00037: ReduceLROnPlateau reducing learning rate to 1.0000000490448793e-21.\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.45791\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.6597 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00038: ReduceLROnPlateau reducing learning rate to 1.0000000692397185e-22.\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.45791\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.6507 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 1.0000000944832675e-23.\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.45791\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6703 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00040: ReduceLROnPlateau reducing learning rate to 1.0000000787060494e-24.\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.45791\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6394 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 1.0000001181490946e-25.\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.45791\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6620 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00042: ReduceLROnPlateau reducing learning rate to 1.0000001428009978e-26.\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.45791\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6760 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.000000142800998e-27.\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.45791\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6701 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 1.0000001235416984e-28.\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.45791\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6521 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00045: ReduceLROnPlateau reducing learning rate to 1.0000001235416985e-29.\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.45791\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6549 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 1.0000001536343539e-30.\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.45791\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.6274 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00047: ReduceLROnPlateau reducing learning rate to 1.000000191250173e-31.\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.45791\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.6443 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 1.0000002147600601e-32.\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.45791\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6443 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.0000002441474188e-33.\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.45791\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.6800 - val_loss: 0.4579\n",
      "\n",
      "Epoch 00050: ReduceLROnPlateau reducing learning rate to 1.0000002074132203e-34.\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.45791\n"
     ]
    }
   ],
   "source": [
    "history=model.fit([Source_inp_tr, target_inp_tr],target_out_tr ,epochs=50,batch_size=256,\n",
    "                  validation_data=([Source_inp_val, target_inp_val], target_out_val),callbacks=[reduce_lr, checkpoint_1, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67liZdF8B7TH"
   },
   "outputs": [],
   "source": [
    "def predict(input_sentence, model):\n",
    "    input_tokens = tokenizer_raw.texts_to_sequences([input_sentence])\n",
    "    input_sequence = pad_sequences(input_tokens, maxlen=max_len, padding='post')\n",
    "\n",
    "  # Getting encoder output and states\n",
    "    enc_initial_states = model.encoder.initialize_states(len(input_sequence))\n",
    "    enc_out, enc_state_h, enc_state_c = model.encoder(input_sequence, enc_initial_states)\n",
    "    state_h, state_c = enc_state_h, enc_state_c\n",
    "    states=[state_h, state_c]\n",
    "    target_word = np.zeros((1,1))\n",
    "    target_word[0,0] = tokenizer_std.word_index['<start>']\n",
    "    stop_condition = False\n",
    "    decoded_sent = ''\n",
    "    pos = 0\n",
    "    while not stop_condition:\n",
    "    # decoder layer, intial states are encoder's final states\n",
    "        output_dec,dec_state_h,dec_state_c =model.layers[1](target_word,states)\n",
    "        output=model.layers[2](output_dec[0])\n",
    "        predicted_id = tf.argmax(output[0]).numpy()\n",
    "        decoded_sent += tokenizer_std.index_word[predicted_id] + ' '\n",
    "        \n",
    "        \n",
    "        # Get all the outputs till we don't get '<end>' word or reached maximum output length\n",
    "        if tokenizer_std.index_word[predicted_id] == '<end>' or len(decoded_sent.split()) >= 20:\n",
    "            stop_condition = True\n",
    "\n",
    "    # current output word is input word for next timestamp\n",
    "        target_word = np.zeros((1,1))\n",
    "        target_word[0,0] = predicted_id\n",
    "\n",
    "    # current out states are input states for next timestamp\n",
    "        state_h, state_c = dec_state_h, dec_state_c\n",
    "        states=[state_h, state_c]\n",
    "\n",
    "        pos += 1\n",
    "\n",
    "    return decoded_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQ3sga4NN64a"
   },
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1695,
     "status": "ok",
     "timestamp": 1620672207665,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "Rsh9qE0FDQKG",
    "outputId": "7f59fd7e-1a8b-48da-a95a-c156a6fd6fc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Wad u want to major?\n",
      "Prediction: i you you you you you you you you you you you you you you you you you you you \n",
      "Actual: What do you want to major? <end>\n",
      "---------------------------------------\n",
      "Input: Yest we try until so long... Today i go sch they oso cant login... Waste our time yest...\n",
      "Prediction: i i you you you you you you you you you you you you you you you you you you \n",
      "Actual: Yesterday we try until so long. Today I go to school, they also can't login. Waste our time yesterday. <end>\n",
      "---------------------------------------\n",
      "Input: They inside e restaurant liao.\n",
      "Prediction: i you you you you you you you you you you you you you you you you you you you \n",
      "Actual: They are inside the restaurant. <end>\n",
      "---------------------------------------\n",
      "Input: Ok,meet you there at 8.45pm, because no taxi and it was raining\n",
      "Prediction: i i you you you you you you you you you you you you you you you you you you \n",
      "Actual: Ok, meet you there at 8:45pm, because no taxi and it was raining. <end>\n",
      "---------------------------------------\n",
      "Input: do u wanna come to my sch? :) but i gotta do proj after sch, for awhile only. i'll not fall sick at tis time la, im strong enough hehe.\n",
      "Prediction: i i you you you you you you you you you you you you you you you you you you \n",
      "Actual: Do you want to come to my school? But I got to do project after school, for a while only. I'll not fall sick at this time, I'm strong enough. <end>\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    input_sentence=tr.Source.iloc[i]\n",
    "    print('Input:',input_sentence)\n",
    "    print('Prediction:',predict(input_sentence, model))\n",
    "    print('Actual:',tr.Target_out.iloc[i])\n",
    "    print('---------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 938,
     "status": "ok",
     "timestamp": 1620672220633,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "VSrsyLrtBuUD",
    "outputId": "d57fafe5-36cb-49ed-db6c-d99b1d2ab9c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4579\n"
     ]
    }
   ],
   "source": [
    "loss=model.evaluate([Source_inp_val, target_inp_val], target_out_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_v4-gGrnOAiK"
   },
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mzelfhzVd6n"
   },
   "source": [
    "<font color='green'>Perplexity is the measure of how well a model predicts the sample. It is defined as nth root over the inverse probabilties of a sentence. \n",
    "\n",
    "<font color='green'>Predicting higher probability for the actual sentences denotes higher performance of the model, as the perplexity is inverse 0f probaiblity, lower the perplexity better the model.\n",
    "\n",
    "<font color='green'>Mathematically, the perplexity can be derived as the exponential of the cross entropy. as the loass we used in our model is crossentropy, we calculate 2^loss to get the perplexity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 956,
     "status": "ok",
     "timestamp": 1620672226113,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "g2LGrrxTA6CA",
    "outputId": "cdafbb41-1777-4bf6-adc4-72a5a6e30a61"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3735523329934292"
      ]
     },
     "execution_count": 222,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://towardsdatascience.com/perplexity-in-language-models-87a196019a94\n",
    "perplexity=2**(loss)\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w46g9NG2OCzc"
   },
   "source": [
    "### Bleu Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gx23F6QVYxPf"
   },
   "source": [
    "<font color='green'>Bleu Score is also a metric for evaluating a generated sentence to a reference sentence. Bleu score ranges from 0 to 1. Higher the score, better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3915,
     "status": "ok",
     "timestamp": 1620672232843,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "JtXhdZqlN4Ml",
    "outputId": "2340ff96-bb4f-40f6-ffc8-9fac756b2b71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "bleu_score=[]\n",
    "for i in range(20):\n",
    "  decoded_sent=predict(val.Source.values[i].split(), model)\n",
    "  bleu_score.append(bleu.sentence_bleu(val.Target_out.values[i].split(), decoded_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 888,
     "status": "ok",
     "timestamp": 1620672252574,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "hKIScHckOcsr",
    "outputId": "79918791-3f4c-4950-e013-cc9b9c682d19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24927488041111542"
      ]
     },
     "execution_count": 225,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_bleu_score=sum(bleu_score)/len(bleu_score)\n",
    "avg_bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4qJrDpiFJXP"
   },
   "source": [
    "# Character_level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R8X9NYydcaXG"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kHcFx-A2ZQre"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('data_cl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "executionInfo": {
     "elapsed": 2856,
     "status": "ok",
     "timestamp": 1620714885432,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "qSie8FbObvpJ",
    "outputId": "e63fa04d-770d-4770-e8f2-1758b7027539"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target_inp</th>\n",
       "      <th>Target_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U wan me to \"chop\" seat 4 u nt?</td>\n",
       "      <td>\\t Do you want me to reserve seat for you or not?</td>\n",
       "      <td>Do you want me to reserve seat for you or not?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yup. U reaching. We order some durian pastry a...</td>\n",
       "      <td>\\t Yeap. You reaching? We ordered some Durian ...</td>\n",
       "      <td>Yeap. You reaching? We ordered some Durian pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>They become more ex oredi... Mine is like 25.....</td>\n",
       "      <td>\\t They become more expensive already. Mine is...</td>\n",
       "      <td>They become more expensive already. Mine is li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I'm thai. what do u do?</td>\n",
       "      <td>\\t I'm Thai. What do you do?</td>\n",
       "      <td>I'm Thai. What do you do?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hi! How did your week go? Haven heard from you...</td>\n",
       "      <td>\\t Hi! How did your week go? Haven't heard fro...</td>\n",
       "      <td>Hi! How did your week go? Haven't heard from y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1988</th>\n",
       "      <td>Hi tina ...</td>\n",
       "      <td>\\t Hi tina.</td>\n",
       "      <td>Hi tina.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989</th>\n",
       "      <td>Hmmm... Thk i usually book on wkends... Depend...</td>\n",
       "      <td>\\t Hmm. I think I usually book on weekends. It...</td>\n",
       "      <td>Hmm. I think I usually book on weekends. It de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>We r near coca oredi...</td>\n",
       "      <td>\\t We are near Coca already.</td>\n",
       "      <td>We are near Coca already.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991</th>\n",
       "      <td>hall Eleven. Got lectures le mah.en forget abt...</td>\n",
       "      <td>\\t Hall eleven. Got lectures. And forget about...</td>\n",
       "      <td>Hall eleven. Got lectures. And forget about co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992</th>\n",
       "      <td>I Bring for u. I can not promise u 100% to win...</td>\n",
       "      <td>\\t I bring for you. I can not promise you 100%...</td>\n",
       "      <td>I bring for you. I can not promise you 100% to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1993 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Source  ...                                         Target_out\n",
       "0                       U wan me to \"chop\" seat 4 u nt?  ...   Do you want me to reserve seat for you or not?\\n\n",
       "1     Yup. U reaching. We order some durian pastry a...  ...  Yeap. You reaching? We ordered some Durian pas...\n",
       "2     They become more ex oredi... Mine is like 25.....  ...  They become more expensive already. Mine is li...\n",
       "3                               I'm thai. what do u do?  ...                        I'm Thai. What do you do?\\n\n",
       "4     Hi! How did your week go? Haven heard from you...  ...  Hi! How did your week go? Haven't heard from y...\n",
       "...                                                 ...  ...                                                ...\n",
       "1988                                        Hi tina ...  ...                                         Hi tina.\\n\n",
       "1989  Hmmm... Thk i usually book on wkends... Depend...  ...  Hmm. I think I usually book on weekends. It de...\n",
       "1990                            We r near coca oredi...  ...                        We are near Coca already.\\n\n",
       "1991  hall Eleven. Got lectures le mah.en forget abt...  ...  Hall eleven. Got lectures. And forget about co...\n",
       "1992  I Bring for u. I can not promise u 100% to win...  ...  I bring for you. I can not promise you 100% to...\n",
       "\n",
       "[1993 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MIuKGHDVZUYp"
   },
   "outputs": [],
   "source": [
    "tr,val = train_test_split(data, test_size=0.05) # as we have very less data to work with, we use major portion to train the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "34e3dcHRZaTe"
   },
   "outputs": [],
   "source": [
    "# Creating the word tokens from the sentences\n",
    "tokenizer_raw = Tokenizer(filters=None, char_level=True, lower=False)\n",
    "tokenizer_raw.fit_on_texts(tr['Source'].values)\n",
    "\n",
    "tokenizer_std = Tokenizer(filters=None, char_level=True, lower=False)\n",
    "tokenizer_std.fit_on_texts(tr['Target_inp'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2GD4g0Ytp3x"
   },
   "outputs": [],
   "source": [
    "#https://www.geeksforgeeks.org/python-string-printable/\n",
    "import string\n",
    "printable_chars=[]\n",
    "for i in string.printable: # Gives the printable characters\n",
    "    printable_chars.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FEBwrC-ugvt"
   },
   "outputs": [],
   "source": [
    "words = dict()\n",
    "for i in range(len(printable_chars)):\n",
    "  words[printable_chars[i]] = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yo5MeTDru5rb"
   },
   "outputs": [],
   "source": [
    "tokenizer_raw.word_index=words\n",
    "tokenizer_std.word_index=words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OVSCUJ8g4w8I"
   },
   "outputs": [],
   "source": [
    "index = dict()\n",
    "for i in range(len(printable_chars)):\n",
    "  index[i+1] = printable_chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BxIm96O65H0n"
   },
   "outputs": [],
   "source": [
    "tokenizer_raw.index_word=index\n",
    "tokenizer_std.index_word=index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ee32GnYPZgmU"
   },
   "outputs": [],
   "source": [
    "Source_vocab_size=len(tokenizer_raw.word_index.keys())+1 \n",
    "Target_vocab_size=len(tokenizer_std.word_index.keys())+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlxxrYbJZmyX"
   },
   "outputs": [],
   "source": [
    "# Assigning token digits to every word in a sentence\n",
    "Source_seq_tr = tokenizer_raw.texts_to_sequences(tr['Source']) \n",
    "Target_inp_seq_tr = tokenizer_std.texts_to_sequences(tr['Target_inp'])\n",
    "Target_out_seq_tr = tokenizer_std.texts_to_sequences(tr['Target_out'])\n",
    "\n",
    "Source_seq_val = tokenizer_raw.texts_to_sequences(val['Source'])\n",
    "Target_inp_seq_val = tokenizer_std.texts_to_sequences(val['Target_inp'])\n",
    "Target_out_seq_val = tokenizer_std.texts_to_sequences(val['Target_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0V_tihXqZq1q"
   },
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in Source_seq_tr:\n",
    "    l.append(len(i))\n",
    "max_len=max(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zsJ4nswZtUn"
   },
   "outputs": [],
   "source": [
    "l=[]\n",
    "for i in Target_inp_seq_tr:\n",
    "    l.append(len(i))\n",
    "max_len_dec_ip=max(l)\n",
    "\n",
    "l=[]\n",
    "for i in Target_out_seq_tr:\n",
    "    l.append(len(i))\n",
    "max_len_dec_op=max(l)\n",
    "\n",
    "max_len_dec=max(max_len_dec_ip,max_len_dec_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6U68ewelZvmS"
   },
   "outputs": [],
   "source": [
    "#Padding the sentences with zeros so that all sentences are of equal length\n",
    "target_inp_tr = pad_sequences(Target_inp_seq_tr,  maxlen=max_len_dec, padding='post')\n",
    "target_out_tr = pad_sequences(Target_out_seq_tr, maxlen=max_len_dec, padding='post')\n",
    "Source_inp_tr = pad_sequences(Source_seq_tr, maxlen=max_len, padding='post')\n",
    "\n",
    "target_inp_val = pad_sequences(Target_inp_seq_val,  maxlen=max_len_dec, padding='post')\n",
    "target_out_val = pad_sequences(Target_out_seq_val, maxlen=max_len_dec, padding='post')\n",
    "Source_inp_val = pad_sequences(Source_seq_val, maxlen=max_len, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NU7GnYYYOLQ4"
   },
   "source": [
    "### Custom Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hV6X45jFZzLe"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units,input_length):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.input_length = input_length\n",
    "        self.enc_units= enc_units\n",
    "        self.lstm_output = 0\n",
    "        self.lstm_state_h=0\n",
    "        self.lstm_state_c=0\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim,trainable=True,input_length=self.input_length,\n",
    "                           mask_zero=True, name=\"embedding_layer_encoder\")\n",
    "        self.lstm = LSTM(self.enc_units, return_state=True, return_sequences=True, name=\"Encoder_LSTM\")\n",
    "        \n",
    "    def call(self, input_sentances, training=True):\n",
    "        input_embedd                           = self.embedding(input_sentances)\n",
    "        self.lstm_output, self.lstm_state_h,self.lstm_state_c = self.lstm(input_embedd)\n",
    "        return self.lstm_output, self.lstm_state_h,self.lstm_state_c\n",
    "\n",
    "    def initialize_states(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.enc_units)), tf.zeros((batch_size, self.enc_units))\n",
    "\n",
    "    def get_states(self):\n",
    "        return self.lstm_state_h,self.lstm_state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKN7aq0gZ1s-"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim,dec_units, input_length):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.dec_units = dec_units\n",
    "        self.input_length = input_length\n",
    "        self.embedding = Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dim,input_length=self.input_length,\n",
    "                           mask_zero=True, name=\"embedding_layer_decoder\",trainable=True)\n",
    "        self.lstm = LSTM(self.dec_units, return_sequences=True, return_state=True, name=\"Encoder_LSTM\")\n",
    "    \n",
    "    def call(self, target_sentances,states):\n",
    "        target_embedd           = self.embedding(target_sentances)\n",
    "        lstm_output,decoder_final_state_h,decoder_final_state_c      = self.lstm(target_embedd, initial_state=states)\n",
    "        return  lstm_output,decoder_final_state_h,decoder_final_state_c "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vj5Slh7Z59f"
   },
   "outputs": [],
   "source": [
    "class Encoder_decoder(Model):\n",
    "    def __init__(self, encoder_inputs_length,decoder_inputs_length, output_vocab_size,batch_size):\n",
    "        super().__init__() # https://stackoverflow.com/a/27134600/4084039\n",
    "        self.encoder = Encoder(vocab_size=Source_vocab_size, embedding_dim=50, input_length=encoder_inputs_length, enc_units=256)\n",
    "        self.decoder = Decoder(vocab_size=Target_vocab_size, embedding_dim=100, input_length=decoder_inputs_length, dec_units=256)\n",
    "        self.dense   = Dense(output_vocab_size)\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "    def call(self, data):\n",
    "        input,output = data[0], data[1]\n",
    "        enc_initial_states = self.encoder.initialize_states(self.batch_size)\n",
    "        encoder_output, encoder_h, encoder_c = self.encoder(input,enc_initial_states)\n",
    "        states=[encoder_h, encoder_c]\n",
    "        decoder_output,decoder_final_state_h,decoder_final_state_c    = self.decoder(output,states)\n",
    "        output                               = self.dense(decoder_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "brsqVQVwab9H"
   },
   "outputs": [],
   "source": [
    "model  = Encoder_decoder(encoder_inputs_length=max_len,decoder_inputs_length=max_len_dec,output_vocab_size=Source_vocab_size, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aGlxZ7JTag_V"
   },
   "outputs": [],
   "source": [
    "def custom_lossfunction(real, pred):\n",
    "    #https://www.tensorflow.org/tutorials/text/nmt_with_attention#define_the_optimizer_and_the_loss_function\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')    \n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.01)\n",
    "model.compile(optimizer=optimizer,loss=custom_lossfunction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnFgC61Mdl-T"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "checkpoint_1 = keras.callbacks.ModelCheckpoint('weights', save_best_only= True, monitor='val_loss', mode = 'min', verbose= 1)\n",
    "tensorboard_callback = keras.callbacks.TensorBoard('log', histogram_freq=0)\n",
    "\n",
    "# Reduce learning rate based on the validation loss\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, mode=\"min\", verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HY6blN47bmjG"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 94263,
     "status": "ok",
     "timestamp": 1620716037081,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "TVcnzT_KKrgU",
    "outputId": "5a32506b-77e4-43f8-91d9-e1f3f265103f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 14s 745ms/step - loss: 0.4104 - val_loss: 0.3773\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.37733, saving model to weights\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 2s 197ms/step - loss: 0.3497 - val_loss: 0.3640\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.37733 to 0.36395, saving model to weights\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.3482 - val_loss: 0.3569\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.36395 to 0.35687, saving model to weights\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 1s 184ms/step - loss: 0.3353 - val_loss: 0.3513\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.35687 to 0.35130, saving model to weights\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.3367 - val_loss: 0.3469\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35130 to 0.34694, saving model to weights\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.3292 - val_loss: 0.3431\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.34694 to 0.34308, saving model to weights\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 1s 184ms/step - loss: 0.3301 - val_loss: 0.3394\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.34308 to 0.33938, saving model to weights\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.3282 - val_loss: 0.3361\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.33938 to 0.33605, saving model to weights\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.3148 - val_loss: 0.3324\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.33605 to 0.33243, saving model to weights\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.3180 - val_loss: 0.3293\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.33243 to 0.32931, saving model to weights\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.3106 - val_loss: 0.3259\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.32931 to 0.32585, saving model to weights\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.3069 - val_loss: 0.3230\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.32585 to 0.32298, saving model to weights\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.3190 - val_loss: 0.3201\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.32298 to 0.32008, saving model to weights\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 1s 184ms/step - loss: 0.3054 - val_loss: 0.3166\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.32008 to 0.31658, saving model to weights\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.2926 - val_loss: 0.3134\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.31658 to 0.31343, saving model to weights\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2988 - val_loss: 0.3110\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.31343 to 0.31099, saving model to weights\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2942 - val_loss: 0.3076\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.31099 to 0.30760, saving model to weights\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2893 - val_loss: 0.3056\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.30760 to 0.30561, saving model to weights\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2891 - val_loss: 0.3021\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.30561 to 0.30207, saving model to weights\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2805 - val_loss: 0.3001\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.30207 to 0.30011, saving model to weights\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 1s 188ms/step - loss: 0.2791 - val_loss: 0.2980\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.30011 to 0.29797, saving model to weights\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2867 - val_loss: 0.2961\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.29797 to 0.29615, saving model to weights\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2669 - val_loss: 0.2938\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.29615 to 0.29375, saving model to weights\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2679 - val_loss: 0.2925\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.29375 to 0.29249, saving model to weights\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2579 - val_loss: 0.2902\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.29249 to 0.29023, saving model to weights\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2611 - val_loss: 0.2889\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.29023 to 0.28890, saving model to weights\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2672 - val_loss: 0.2871\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.28890 to 0.28706, saving model to weights\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2531 - val_loss: 0.2860\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.28706 to 0.28599, saving model to weights\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 0.2552 - val_loss: 0.2856\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.28599 to 0.28558, saving model to weights\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2518 - val_loss: 0.2847\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.28558 to 0.28471, saving model to weights\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2538 - val_loss: 0.2827\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.28471 to 0.28267, saving model to weights\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2538 - val_loss: 0.2815\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.28267 to 0.28146, saving model to weights\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2400 - val_loss: 0.2809\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.28146 to 0.28090, saving model to weights\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2384 - val_loss: 0.2806\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.28090 to 0.28065, saving model to weights\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2445 - val_loss: 0.2790\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.28065 to 0.27903, saving model to weights\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 2s 188ms/step - loss: 0.2448 - val_loss: 0.2789\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.27903 to 0.27886, saving model to weights\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 2s 188ms/step - loss: 0.2398 - val_loss: 0.2777\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.27886 to 0.27766, saving model to weights\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2366 - val_loss: 0.2779\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.27766\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 1s 189ms/step - loss: 0.2300 - val_loss: 0.2779\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.27766\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 0.2297 - val_loss: 0.2778\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.27766\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2269 - val_loss: 0.2770\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.27766 to 0.27702, saving model to weights\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2235 - val_loss: 0.2773\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.27702\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 1s 188ms/step - loss: 0.2204 - val_loss: 0.2782\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.27702\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 0.2192 - val_loss: 0.2770\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.27702 to 0.27701, saving model to weights\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2223 - val_loss: 0.2782\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.27701\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2161 - val_loss: 0.2775\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 0.0009999999776482583.\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.27701\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2133 - val_loss: 0.2759\n",
      "\n",
      "Epoch 00047: val_loss improved from 0.27701 to 0.27591, saving model to weights\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 2s 189ms/step - loss: 0.2040 - val_loss: 0.2761\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.27591\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2145 - val_loss: 0.2765\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.27591\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2127 - val_loss: 0.2763\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.27591\n"
     ]
    }
   ],
   "source": [
    "history=model.fit([Source_inp_tr, target_inp_tr],target_out_tr ,epochs=50,batch_size=256,\n",
    "                  validation_data=([Source_inp_val, target_inp_val], target_out_val),callbacks=[reduce_lr, checkpoint_1, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77430,
     "status": "ok",
     "timestamp": 1620716136677,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "yLCtIZWPNDLu",
    "outputId": "f19b32ca-7b90-46c5-8f2c-fd50b11f30bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 3s 345ms/step - loss: 0.2072 - val_loss: 0.2762\n",
      "\n",
      "Epoch 00001: val_loss did not improve from 0.27591\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2068 - val_loss: 0.2765\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.27591\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2064 - val_loss: 0.2766\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.27591\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2062 - val_loss: 0.2768\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.27591\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2055 - val_loss: 0.2770\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.27591\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2053 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00006: ReduceLROnPlateau reducing learning rate to 9.999999310821295e-05.\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.27591\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2045 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.27591\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2048 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.27591\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2047 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.27591\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2050 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.27591\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2044 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 9.999999019782991e-06.\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.27591\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 1s 184ms/step - loss: 0.2043 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.27591\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2044 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.27591\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2044 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.27591\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2042 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.27591\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2043 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-07.\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.27591\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2042 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.27591\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2043 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.27591\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2042 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.27591\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2049 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.27591\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2044 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00021: ReduceLROnPlateau reducing learning rate to 9.99999883788405e-08.\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.27591\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2044 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.27591\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2044 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.27591\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2042 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.27591\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2046 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.27591\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2044 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00026: ReduceLROnPlateau reducing learning rate to 9.999998695775504e-09.\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.27591\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2043 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.27591\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 1s 184ms/step - loss: 0.2046 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.27591\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2043 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.27591\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2043 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.27591\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2045 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 9.99999905104687e-10.\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.27591\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2046 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.27591\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 1s 188ms/step - loss: 0.2042 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.27591\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 1s 184ms/step - loss: 0.2044 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.27591\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2047 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.27591\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2043 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.999998606957661e-11.\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.27591\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2045 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.27591\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2042 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.27591\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2042 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.27591\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2046 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.27591\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 2s 190ms/step - loss: 0.2042 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00041: ReduceLROnPlateau reducing learning rate to 9.99999874573554e-12.\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.27591\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2046 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.27591\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2042 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.27591\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 1s 188ms/step - loss: 0.2043 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.27591\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2044 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.27591\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 1s 185ms/step - loss: 0.2043 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00046: ReduceLROnPlateau reducing learning rate to 9.999999092680235e-13.\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.27591\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2044 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.27591\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2045 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.27591\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 1s 186ms/step - loss: 0.2043 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.27591\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 1s 187ms/step - loss: 0.2044 - val_loss: 0.2771\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.27591\n"
     ]
    }
   ],
   "source": [
    "history=model.fit([Source_inp_tr, target_inp_tr],target_out_tr ,epochs=50,batch_size=256,\n",
    "                  validation_data=([Source_inp_val, target_inp_val], target_out_val),callbacks=[reduce_lr, checkpoint_1, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXQ93TGlITZo"
   },
   "outputs": [],
   "source": [
    "def predict(input_sentence,model):\n",
    "\n",
    "  input_sequence=tokenizer_raw.texts_to_sequences([input_sentence])\n",
    "  \n",
    "\n",
    "  inputs=pad_sequences(input_sequence,maxlen=170,padding='post')\n",
    "  inputs=tf.convert_to_tensor(inputs)\n",
    "  result=''\n",
    "  units=100\n",
    "  hidden=[tf.zeros((1,units))]\n",
    "  encoder_output,hidden_state,cell_state=model.encoder(inputs,hidden)\n",
    "  dec_hidden=hidden_state\n",
    "  dec_input=tf.expand_dims([tokenizer_std.word_index['\\t']],0)\n",
    "  for t in range(202):\n",
    "      predictions,dec_hidden,cell_state,=model.layers[1](dec_input,[dec_hidden,cell_state])\n",
    "      output=model.layers[2](predictions[0])\n",
    "      predicted_id=tf.argmax(output[0]).numpy()\n",
    "      result+=tokenizer_std.index_word[predicted_id]\n",
    "      if tokenizer_std.word_index['\\n']==predicted_id:\n",
    "          return result\n",
    "      dec_input= tf.expand_dims([predicted_id],0)\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Peb0wURkbqT8"
   },
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3089,
     "status": "ok",
     "timestamp": 1620735873876,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "ywnBDyEeYpYw",
    "outputId": "36728ab6-2a2c-4cdb-c0a9-18d7a81231ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hey i think i will go cut lah. U think can get slot at 2 ?\n",
      "Hey I think I will go and cut. You think can get slot at 2?\n",
      "\n",
      "HIII o o o o o o o o o o o o o o o o o o o o o o o o o o o o o?Ta?T\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_sentence = tr.Source.iloc[1]\n",
    "print(input_sentence)\n",
    "print(tr.Target_out.iloc[1])\n",
    "print(predict(input_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1215,
     "status": "ok",
     "timestamp": 1620716152072,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "luxMpPOeDO1E",
    "outputId": "ef3f395f-2031-4980-b1cd-4e8883a2d408"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'WIhZ tae.Ya.Ya\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentence = 'watch wat'\n",
    "predict(input_sentence, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "__G4kOnXOXLZ"
   },
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2229,
     "status": "ok",
     "timestamp": 1620716158995,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "tZsJDwXQazkp",
    "outputId": "34e8b791-3225-4453-9720-f4775749350a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 39ms/step - loss: 0.2777\n"
     ]
    }
   ],
   "source": [
    "loss=model.evaluate([Source_inp_val, target_inp_val], target_out_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1775,
     "status": "ok",
     "timestamp": 1620716162084,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "Kt4DZYrtbZQ3",
    "outputId": "33ee5a73-ee97-4868-a484-e6b4ad3096d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2122830038899854"
      ]
     },
     "execution_count": 61,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://towardsdatascience.com/perplexity-in-language-models-87a196019a94\n",
    "perplexity=2**(loss)\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtMnYobcOZ6A"
   },
   "source": [
    "### Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4960,
     "status": "ok",
     "timestamp": 1620716168170,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "NUwRRfTPqD_5",
    "outputId": "ccbb8f3c-7a9c-4a17-ed60-ff9d2d853076"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n",
      "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
      "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
      "BLEU scores might be undesirable; use SmoothingFunction().\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "import nltk.translate.bleu_score as bleu\n",
    "bleu_score=[]\n",
    "for i in range(20):\n",
    "  decoded_sent=predict(val.Source.values[i].split(), model)\n",
    "  bleu_score.append(bleu.sentence_bleu(val.Target_out.values[i].split(), decoded_sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28366,
     "status": "ok",
     "timestamp": 1620727557871,
     "user": {
      "displayName": "Deepu S",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhA3jWBdgJvxz0fRJHDdC-2zr5r_I2FAAyJ57Yw81s=s64",
      "userId": "08960752430539190859"
     },
     "user_tz": -330
    },
    "id": "0HqWJtpiqD_6",
    "outputId": "80e8935f-1ccf-4089-8be4-c6812013c0a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3746864939524901"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_bleu_score=sum(bleu_score)/len(bleu_score)\n",
    "avg_bleu_score"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Baseline_Model_CS2.ipynb",
   "provenance": [
    {
     "file_id": "1IzPbxNFQmmSYk9s14L4YjBfUgACn9mW2",
     "timestamp": 1620352650057
    },
    {
     "file_id": "1G7NKeneJNyRtcRxLVbbF9jYtRyuTOa-R",
     "timestamp": 1592749700622
    },
    {
     "file_id": "https://github.com/satyajitghana/TSAI-DeepVision-EVA4.0/blob/master/Utils/Colab_25GBRAM_GPU.ipynb",
     "timestamp": 1592043804148
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
